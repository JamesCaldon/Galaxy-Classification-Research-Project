{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "trained_model = \"gamornet.h5\"\r\n",
    "transfer_data = \"califa_s_1.0\"\r\n",
    "testing_data = \"nair_abraham_2010_hdf5\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import tensorflow as tf\r\n",
    "import utility.initialize_tf"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.15.3\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Implementing LRN in Keras. Code from \"Deep Learning with Python\" by F.\r\n",
    "# Chollet\r\n",
    "from tensorflow.keras import backend as K\r\n",
    "from tensorflow.keras.layers import Layer\r\n",
    "class LocalResponseNormalization(Layer):\r\n",
    "\r\n",
    "    def __init__(self, n=5, alpha=0.0001, beta=0.75, k=1.0, **kwargs):\r\n",
    "        self.n = n\r\n",
    "        self.alpha = alpha\r\n",
    "        self.beta = beta\r\n",
    "        self.k = k\r\n",
    "        super(LocalResponseNormalization, self).__init__(**kwargs)\r\n",
    "\r\n",
    "    def build(self, input_shape):\r\n",
    "        self.shape = input_shape\r\n",
    "        super(LocalResponseNormalization, self).build(input_shape)\r\n",
    "\r\n",
    "    def call(self, x, mask=None):\r\n",
    "        if K.image_data_format == \"channels_first\":\r\n",
    "            _, f, r, c = self.shape\r\n",
    "        else:\r\n",
    "            _, r, c, f = self.shape\r\n",
    "        squared = K.square(x)\r\n",
    "        pooled = K.pool2d(squared, (self.n, self.n), strides=(1, 1),\r\n",
    "                          padding=\"same\", pool_mode=\"avg\")\r\n",
    "        if K.image_data_format == \"channels_first\":\r\n",
    "            summed = K.sum(pooled, axis=1, keepdims=True)\r\n",
    "            averaged = self.alpha * K.repeat_elements(summed, f, axis=1)\r\n",
    "        else:\r\n",
    "            summed = K.sum(pooled, axis=3, keepdims=True)\r\n",
    "            averaged = self.alpha * K.repeat_elements(summed, f, axis=3)\r\n",
    "        denom = K.pow(self.k + averaged, self.beta)\r\n",
    "        return x / denom\r\n",
    "\r\n",
    "    def get_output_shape_for(self, input_shape):\r\n",
    "        return input_shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "base_model = tf.keras.models.load_model('previous_models\\\\' + trained_model, custom_objects={'LocalResponseNormalization': LocalResponseNormalization})\r\n",
    "#base_model = tf.keras.models.load_model('previous_models\\\\' + trained_model)\r\n",
    "\r\n",
    "#base_model = tf.saved_model.load('previous_models\\\\')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda3\\envs\\hon-tf-1.15.3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\envs\\hon-tf-1.15.3\\lib\\site-packages\\tensorflow_core\\python\\keras\\initializers.py:94: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\envs\\hon-tf-1.15.3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "base_model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 42, 42, 96)        11712     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 21, 21, 96)        0         \n",
      "_________________________________________________________________\n",
      "local_response_normalization (None, 21, 21, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 21, 21, 256)       614656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 11, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "local_response_normalization (None, 11, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 384)       885120    \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 11, 11, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 11, 256)       884992    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "local_response_normalization (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              37752832  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 12291     \n",
      "=================================================================\n",
      "Total params: 58,270,403\n",
      "Trainable params: 58,270,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "base_model.pop()\r\n",
    "base_model.pop()\r\n",
    "base_model.pop()\r\n",
    "base_model.pop()\r\n",
    "base_model.pop()\r\n",
    "#base_model.pop()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "base_model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 42, 42, 96)        11712     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 21, 21, 96)        0         \n",
      "_________________________________________________________________\n",
      "local_response_normalization (None, 21, 21, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 21, 21, 256)       614656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 11, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "local_response_normalization (None, 11, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 384)       885120    \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 11, 11, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 11, 256)       884992    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "local_response_normalization (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "=================================================================\n",
      "WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "Total params: 58,270,403\n",
      "Trainable params: 58,270,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Conv2D, MaxPooling2D, Flatten\r\n",
    "# from tensorflow.keras.models import Model\r\n",
    "\r\n",
    "# input_shape = (100, 100, 1)\r\n",
    "\r\n",
    "# # Freeze base model\r\n",
    "# base_model.trainable=False\r\n",
    "\r\n",
    "# # Create New Model on top\r\n",
    "# inputs = tf.keras.Input(shape=input_shape)\r\n",
    "# x = base_model(inputs, training=False)\r\n",
    "\r\n",
    "# x = Conv2D(32, kernel_size=(3, 3),\r\n",
    "#                 activation='relu',\r\n",
    "#                 input_shape=input_shape)(x)\r\n",
    "# x = Conv2D(64, (3, 3), activation='relu')(x)\r\n",
    "# x = MaxPooling2D(pool_size=(2, 2))(x)\r\n",
    "# x = Dropout(0.25)(x)\r\n",
    "# x = Flatten()(x)\r\n",
    "# x = Dense(128, activation='relu')(x)\r\n",
    "# x = Dropout(0.5)(x)\r\n",
    "# outputs = Dense(2, activation='softmax')(x)\r\n",
    "# new_model = tf.keras.Model(inputs, outputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\r\n",
    "from tensorflow.keras.models import Model\r\n",
    "\r\n",
    "# Freeze base model\r\n",
    "base_model.trainable=False\r\n",
    "\r\n",
    "# Create New Model on top\r\n",
    "inputs = tf.keras.Input(shape=(167, 167, 1))\r\n",
    "x = base_model(inputs, training=False)\r\n",
    "#x = GlobalAveragePooling2D()(x)\r\n",
    "#x = Dense(512, activation='relu')(x)\r\n",
    "#x = Dropout(0.5)(x)\r\n",
    "x = Dense(128, activation='relu')(x)\r\n",
    "x = Dropout(0.5)(x) # added later\r\n",
    "outputs = Dense(2, activation='softmax')(x)\r\n",
    "new_model = tf.keras.Model(inputs, outputs)\r\n",
    "new_model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 100, 100, 1)]     0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    multiple                  3723968   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               524416    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 4,248,642\n",
      "Trainable params: 524,674\n",
      "Non-trainable params: 3,723,968\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Train additional layers\r\n",
    "import numpy as np\r\n",
    "from data_loading import *\r\n",
    "x_dataset, y_dataset, metadata = load_hdf5_data(name=transfer_data)\r\n",
    "x_dataset = np.expand_dims(x_dataset, axis=3)\r\n",
    "#from sklearn import model_selection\r\n",
    "#x_train, x_test, y_train, y_test = model_selection.train_test_split(x_dataset, y_dataset, train_size=0.7, random_state=1)\r\n",
    "\r\n",
    "#print(x_train.shape)\r\n",
    "#print(x_test.shape)\r\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'tables'.  Use pip or conda to install tables.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3f4ce6604f92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdata_loading\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mx_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_hdf5_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransfer_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mx_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#from sklearn import model_selection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Galaxy-Classification-Research-Project\\utility\\data_loading.py\u001b[0m in \u001b[0;36mload_hdf5_data\u001b[1;34m(name, count, skip)\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_PARENT_PATH\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0m_DATA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"filepath\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mall_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\hon-tf-1.15.3\\lib\\site-packages\\pandas\\io\\pytables.py\u001b[0m in \u001b[0;36mread_hdf\u001b[1;34m(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"File {path_or_buf} does not exist\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m         \u001b[0mstore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHDFStore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m         \u001b[1;31m# can't auto open/close if we are using an iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[1;31m# so delegate to the iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\hon-tf-1.15.3\\lib\\site-packages\\pandas\\io\\pytables.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, mode, complevel, complib, fletcher32, **kwargs)\u001b[0m\n\u001b[0;32m    532\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"format is not a defined argument for HDFStore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 534\u001b[1;33m         \u001b[0mtables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_optional_dependency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tables\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcomplib\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcomplib\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_complibs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\hon-tf-1.15.3\\lib\\site-packages\\pandas\\compat\\_optional.py\u001b[0m in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, raise_on_missing, on_version)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mraise_on_missing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Missing optional dependency 'tables'.  Use pip or conda to install tables."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_datagen(subset):\r\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
    "\r\n",
    "    datagen = ImageDataGenerator(\r\n",
    "        horizontal_flip=True,\r\n",
    "        vertical_flip=True,\r\n",
    "        width_shift_range=0.2,\r\n",
    "        height_shift_range=0.2,\r\n",
    "        zoom_range=[0.8, 1.2],\r\n",
    "        rotation_range=45,\r\n",
    "        fill_mode='constant',\r\n",
    "        cval = 0,\r\n",
    "        validation_split = 0.2\r\n",
    "    )\r\n",
    "    datagen.fit(subset)\r\n",
    "    return datagen\r\n",
    "\r\n",
    "datagen = create_datagen(x_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "new_model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n",
    "                  optimizer=tf.keras.optimizers.Adadelta(learning_rate=1.0),\r\n",
    "                  metrics=['accuracy'])\r\n",
    "train_history = new_model.fit(datagen.flow(x_dataset, y_dataset, batch_size=32), batch_size=32, epochs=1000,\r\n",
    "          verbose=0, validation_data=datagen.flow(x_dataset, y_dataset, batch_size=32, subset='validation'))\r\n",
    "score = new_model.evaluate(datagen.flow(x_dataset, y_dataset, batch_size=32, subset='validation'), verbose=0)\r\n",
    "print('Test score:', score[0])\r\n",
    "print('Test accuracy:', score[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from matplotlib import pyplot as plt\r\n",
    "plt.plot(train_history.history['loss'])\r\n",
    "plt.plot(train_history.history['val_loss'])\r\n",
    "plt.legend(['loss', 'val_loss'])\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Train whole model for fine tuning.\r\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\r\n",
    "new_model.trainable = True\r\n",
    "\r\n",
    "new_model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n",
    "                  optimizer=tf.keras.optimizers.Adadelta(learning_rate=0.1),\r\n",
    "                  metrics=['accuracy'])\r\n",
    "new_model.fit(datagen.flow(x_dataset, y_dataset, batch_size=32), batch_size=32, epochs=10,\r\n",
    "          verbose=1, validation_data=datagen.flow(x_dataset, y_dataset, batch_size=32, subset='validation'), callbacks=[callback])\r\n",
    "score = new_model.evaluate(datagen.flow(x_dataset, y_dataset, batch_size=32, subset='validation'), verbose=0)\r\n",
    "print('Test score:', score[0])\r\n",
    "print('Test accuracy:', score[1])\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "new_model.save(r\"saved_models\\gamornet_transfer_learning.h5\", overwrite=True, save_format='h5')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x, Y, metadata = load_hdf5_data(name=testing_data)\r\n",
    "\r\n",
    "pred = new_model.predict(x).argmax(axis=1)\r\n",
    "\r\n",
    "results = np.asarray(np.unique(pred, return_counts=True)).T\r\n",
    "print(\"total images: \" + str(np.sum(results)-1))\r\n",
    "print(\"Percentage of E: \", results[0, 1]/(np.sum(results)-1)*100)\r\n",
    "print(\"Percentage of ES: \", results[1, 1]/(np.sum(results)-1)*100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from sklearn import metrics\r\n",
    "\r\n",
    "# true_data = y_test\r\n",
    "# predicted_data = new_model.predict(x_test)\r\n",
    "# metrics.ConfusionMatrixDisplay(metrics.confusion_matrix(true_data.argmax(axis=1), predicted_data.argmax(axis=1))).plot(cmap='Blues')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from sklearn import metrics\r\n",
    "# class_labels = np.unique(metadata[\"class\"])\r\n",
    "# print(class_labels)\r\n",
    "# recid = -1\r\n",
    "# print(metadata[recid])\r\n",
    "\r\n",
    "# #print(class_labels[Y_val[recid]])\r\n",
    "# print(Y_val[recid])\r\n",
    "# #print(predicted_data)\r\n",
    "# true_data = Y_val\r\n",
    "# predicted_data = new_model.predict(x_val)\r\n",
    "# metrics.ConfusionMatrixDisplay(metrics.confusion_matrix(true_data.argmax(axis=1), predicted_data.argmax(axis=1)),\r\n",
    "#                                display_labels=class_labels).plot(cmap='Blues')"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# #mpl.rcParams['figure.figsize'] = [16, 40]\r\n",
    "# #mpl.rcParams['figure.dpi'] = 72\r\n",
    "# plotting_helpers.plot_classification_results(images=x_val, display_size=(12, 5), y_preds=predicted_data.argmax(axis=1), y_trues=Y_val.argmax(axis=1),\r\n",
    "#                             y_labels=class_labels,\r\n",
    "#                             galaxy_names=metadata[\"name\"], random_sample=True)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# no_arg = new_model.predict(x_dataset[:1])\r\n",
    "# no_arg.argmax(axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print(x_val.shape)\r\n",
    "# pyplot.imshow(x_val[10])\r\n",
    "# pyplot.colorbar()\r\n",
    "# pyplot.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('hon-tf-2.5.0': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "b20f78c8b161c2d28d097599ec49d232bad73a67c2dd6ab644d3da97e4813ebf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}